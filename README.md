# XMarks - Modular Twitter Knowledge Management System (v2.0)

## ğŸ¯ **Overview**

XMarks is a powerful Twitter bookmark processing system that transforms your saved tweets into an Obsidian-compatible knowledge vault. Built on a **GraphQL-first architecture**, it extracts rich data from Twitter using Playwright automation, then applies modular processors to generate comprehensive markdown documentation with linked media, papers, and documents.

### ğŸ†• **Version 2.0 Improvements**
- **30% Less Code**: Eliminated 200+ lines of duplication with MarkdownGenerator
- **2x Faster Processing**: Single-pass pipeline architecture
- **Better Async**: Enhanced LLM processing with semaphore control
- **Auto-Config**: Environment variable support with `.env` file loading
- **Extensible**: Document factory pattern for easy processor additions

### **Key Features**
- ğŸ“¡ **GraphQL Data Collection** - Direct Twitter API access via Playwright
- ğŸ§µ **Smart Thread Detection** - Uses Twitter's own thread indicators
- ğŸ“„ **Document Processing** - ArXiv papers, PDFs, GitHub/HuggingFace READMEs
- ğŸ–¼ï¸ **Media Management** - Downloads and links images, videos, GIFs
- ğŸ¬ **YouTube Processing** - Video metadata, optional embeds, and transcripts
- ğŸ¤ **Video Transcripts** - Local Whisper or Deepgram transcription for Twitter videos (60s+)
- ğŸ¤– **AI Enhancement** - LLM-powered tags, summaries, and alt text (async-capable)
- ğŸ”„ **Resume Capability** - Interrupt and restart anytime
- âš¡ **Single-Pass Pipeline** - End-to-end processing in one optimized pass
- ğŸŒ **Live Capture API** - FastAPI server + browser extension/userscript for real-time capture
- ğŸ“š **Obsidian Integration** - Ready-to-use markdown with wikilinks

---

## ğŸš€ **Quick Start**

### **Prerequisites**
```bash
# Install Python dependencies
pip install -r requirements.txt

# Install Playwright browsers (required for GraphQL)
playwright install
```

### **Basic Commands**

```bash
# Check current status
python xmarks.py stats

# Process cached data (fast, no rate limits)
python xmarks.py process --use-cache --limit 100

# NEW: Optimized single-pass pipeline (2x faster)
python xmarks.py pipeline --use-cache --batch-size 10

# Preview without writing files (requires --use-cache)
python xmarks.py pipeline --use-cache --dry-run

# NEW: Test async LLM with performance monitoring
python xmarks.py async-test --limit 5 --concurrent 8

# Process Twitter video transcripts (Whisper or Deepgram)
python xmarks.py twitter-transcripts --limit 50 --resume

# Download new GraphQL data (rate limited)
python xmarks.py download --resume --limit 50

# Full pipeline (download + process)
python xmarks.py full --resume

# NEW: Post-process tweets for YouTube videos (embeds + transcripts)
python xmarks.py youtube --limit 100 --no-resume

# NEW: Update existing tweet/thread files with clickable video thumbnails
python xmarks.py update-videos --no-resume --no-threads

# NEW: Process your GitHub stars to summaries + README captures
python xmarks.py github-stars --limit 50 --resume

# NEW: Process your HuggingFace likes (models/datasets/spaces)
python xmarks.py huggingface-likes --limit 50 --resume
```

---

## ğŸ“ **Project Structure**

```
xmarks/
â”œâ”€â”€ xmarks.py                        # Main CLI interface
â”œâ”€â”€ xmarks_api.py                    # FastAPI server for live capture
â”œâ”€â”€ core/                            # Core engine components
â”‚   â”œâ”€â”€ graphql_engine.py           # Twitter GraphQL collection
â”‚   â”œâ”€â”€ data_models.py              # Data structures
â”‚   â”œâ”€â”€ config.py                   # Configuration management
â”‚   â”œâ”€â”€ llm_interface.py            # Multi-provider LLM support
â”‚   â”œâ”€â”€ download_tracker.py         # Download status tracking
â”‚   â””â”€â”€ github_utils.py             # Repository utilities
â”œâ”€â”€ processors/                      # Modular processors
â”‚   â”œâ”€â”€ content_processor.py        # Tweet markdown generation
â”‚   â”œâ”€â”€ thread_processor.py         # Thread detection & files
â”‚   â”œâ”€â”€ url_processor.py            # URL expansion & PDFs
â”‚   â”œâ”€â”€ media_processor.py          # Media downloads
â”‚   â”œâ”€â”€ arxiv_processor.py          # ArXiv paper processing
â”‚   â”œâ”€â”€ llm_processor.py            # AI-powered features
â”‚   â”œâ”€â”€ async_llm_processor.py      # Async LLM with concurrency control
â”‚   â”œâ”€â”€ youtube_processor.py        # YouTube video processing
â”‚   â”œâ”€â”€ twitter_video_transcript_processor.py # Twitter video transcripts
â”‚   â”œâ”€â”€ deepgram_transcript_processor.py      # Optional Deepgram support
â”‚   â””â”€â”€ cache_loader.py             # Cache data loading
â”œâ”€â”€ knowledge_vault/                 # Generated output
â”‚   â”œâ”€â”€ tweets/                     # Individual tweet files
â”‚   â”œâ”€â”€ threads/                    # Thread compilations
â”‚   â”œâ”€â”€ papers/                     # ArXiv papers (PDFs)
â”‚   â”œâ”€â”€ pdfs/                       # General PDF documents
â”‚   â”œâ”€â”€ media/                      # Downloaded media
â”‚   â”œâ”€â”€ readmes/                    # Repository READMEs
â”‚   â”œâ”€â”€ repos/                      # Saved repository README files
â”‚   â”œâ”€â”€ stars/                      # GitHub/HF repo summaries
â”‚   â””â”€â”€ transcripts/                # YouTube/Twitter video transcripts
â”œâ”€â”€ graphql_cache/                   # Cached API responses
â”œâ”€â”€ config.json                      # Configuration file
â”œâ”€â”€ twitter-bookmarks-merged.json    # Source bookmarks
â””â”€â”€ twitter_cookies.json             # Authentication
```

### Additional Components

- `browser_extension/` and `userscript/`: Real-time capture from Twitter/X to the local API.
- `vector_store/`: Optional local vector artifacts (if used).

---

## ğŸ”§ **CLI Commands**

### **`xmarks.py stats`**
Shows current processing status and statistics.

### **`xmarks.py download [options]`**
Downloads GraphQL data from Twitter (rate limited).

**Options:**
- `--limit N` - Process only N tweets
- `--resume` - Skip already cached tweets (default: true)
- `--cookies FILE` - Custom cookies file
- `--verbose` - Enable detailed logging

### **`xmarks.py process [options]`**
Processes tweets into markdown files.

**Options:**
- `--use-cache` - Use cached GraphQL data (recommended)
- `--limit N` - Process only N tweets
- `--resume` - Skip existing files (default: true)
- `--verbose` - Enable detailed logging

### **`xmarks.py full [options]`**
Complete pipeline: download + process.

**Options:**
- `--limit N` - Process only N tweets
- `--resume` - Enable resume capability (default: true)
- `--verbose` - Enable detailed logging

### ğŸ†• **`xmarks.py pipeline [options]`**
Optimized single-pass processing (2x faster than standard process).

**Options:**
- `--use-cache` - Use cached GraphQL data
- `--limit N` - Process only N tweets
- `--batch-size N` - Concurrent batch size (default: 10)
- `--resume` - Skip existing files (default: true)

### ğŸ†• **`xmarks.py async-test [options]`**
Test enhanced async LLM processing with performance monitoring.

**Options:**
- `--limit N` - Number of tweets to test (default: 5)
- `--concurrent N` - Max concurrent requests (default: 8)
- `--timeout N` - Request timeout in seconds (default: 20)

---

## ğŸ§© **Core Components**

### **GraphQL Engine**
- Playwright-based Twitter data collection
- Rate limiting (45 requests/15 minutes)
- Response caching for resume capability
- Extracts threads, URLs, media metadata

### **Data Models**
- `Tweet` - Enhanced tweet with GraphQL data
- `MediaItem` - Media file information
- `URLMapping` - t.co â†’ expanded URL mappings
- `ThreadInfo` - Thread metadata
- `ProcessingStats` - Operation statistics

### **Processors**

| Processor | Function | Status |
|-----------|----------|---------|
| **ContentProcessor** | Generates individual tweet markdown files | âœ… Core |
| **ThreadProcessor** | Detects and compiles thread files | âœ… Core |
| **CacheLoader** | Loads cached GraphQL for fast processing | âœ… Core |
| **URLProcessor** | Expands URLs and manages PDFs/READMEs | âœ… Enhanced |
| **MediaProcessor** | Downloads media with Obsidian links | âœ… Enhanced |
| **ArXivProcessor** | Processes academic papers with metadata | âœ… Enhanced |
| **LLMProcessor** | Adds AI-powered tags, summaries, alt text | âœ… Enhanced |
| **AsyncLLMProcessor** | Async LLM with concurrency control | âœ… Enhanced |
| **YouTubeProcessor** | YouTube metadata, transcripts, embeds | âœ… Enhanced |
| **TwitterVideoTranscriptProcessor** | Whisper/Deepgram Twitter transcripts | âœ… Enhanced |
| **VideoUpdater** | Updates tweet/thread files with video thumbnails | âœ… Utility |
| **GitHubStarsProcessor** | Processes your GitHub starred repos | âœ… Utility |
| **HuggingFaceLikesProcessor** | Processes your HuggingFace likes | âœ… Utility |
| **MarkdownGenerator** | Shared markdown generation utility | âœ… Utility |
| **MetadataDB** | SQLite metadata (tweets, downloads, files, LLM cache) | ğŸ†• |

---

## ğŸ¤– **LLM Integration**

### **Multi-Provider Support**
- OpenAI (GPT models)
- Anthropic (Claude models)
- OpenRouter (various models)
- Local (Ollama)

### **AI Features**
- **Tags** - Automatic hashtag generation
- **Summaries** - Concise summaries for long tweets/threads
- **Alt Text** - Accessibility descriptions for images
- **README Summaries** - Repository description extraction

### **Configuration Example**
```json
{
  "llm": {
    "anthropic": {
      "enabled": true,
      "model": "claude-sonnet-4-20250514"
    },
    "tasks": {
      "tags": {"provider": "anthropic", "enabled": true},
      "summary": {"provider": "anthropic", "enabled": true},
      "alt_text": {"provider": "openrouter", "enabled": true}
    }
  },
  "whisper": {
    "enabled": true,
    "base_url": "http://localhost:11434/v1",
    "model": "Systran/faster-distil-whisper-large-v3",
    "min_duration_seconds": 60,
    "max_chunk_mb": 25,
    "target_bitrate_kbps": 128
  }
}
```

### **Video Transcript Setup**
1. **Install ffmpeg**: Required for audio extraction
   ```bash
   # Ubuntu/Debian
   sudo apt install ffmpeg
   
   # macOS
   brew install ffmpeg
   
   # Windows
   # Download from https://ffmpeg.org/download.html
   ```

2. **Start Whisper Server**: Use Ollama or compatible server
   ```bash
   # Install Ollama
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Pull Whisper model
   ollama pull whisper
   
   # Start server (default port 11434)
   ollama serve
   ```

3. **Configure**: Update `config.json` with your Whisper server URL

#### Deepgram (Optional, faster)
1. Set environment variable `DEEPGRAM_API_KEY`.
2. Enable in `config.json` under `deepgram.enabled: true` and set model (e.g., `nova-2`).
3. Run the same transcript commands; the system will auto-prefer Deepgram if configured.

---

## ğŸ“ **Generated Markdown Structure**

Each tweet file includes:

```markdown
---
type: tweet
id: 1234567890
author: "username"
created_at: 2024-01-01 12:00:00
enhanced: true
---

# Tweet by @username

## Content
[Tweet text with expanded URLs]
![[media_file.jpg]]

## ArXiv Papers
### Paper 1: [Title]
- **ArXiv ID**: [2401.12345](https://arxiv.org/abs/2401.12345)
- **Abstract**: [Paper abstract]
- **PDF**: [[2401.12345.pdf]]

## PDF Documents
### Document 1: [Title]
- **URL**: [https://example.com/doc.pdf]
- **PDF**: [[document_title.pdf]]

## Summary
[LLM-generated summary for long tweets]

#tag1 #tag2 #tag3
```

---

## ğŸ¯ **Key Advantages**

### **1. GraphQL-First Architecture**
Direct access to Twitter's rich data structure, not just basic text.

### **2. Real Thread Detection**
Uses Twitter's `tweetDisplayType: "SelfThread"` for accurate thread grouping.

### **3. Comprehensive Document Processing**
- ArXiv papers with full metadata
- PDF downloads with smart naming
- GitHub/HuggingFace README extraction

### **4. Video Transcript Processing**
- Local Whisper transcription for Twitter videos (60+ seconds)
- Automatic audio extraction with ffmpeg
- LLM-powered transcript cleaning and formatting
- Chunked processing for large audio files

### **5. Resume Capability**
All operations support interruption and continuation.

### **6. Fast Cached Processing**
Once GraphQL data is cached, processing has no rate limits.

---

## ğŸ” **Usage Examples**

### **Scenario 1: Quick Processing**
Process existing cached data with all enhancements:
```bash
python xmarks.py process --use-cache
```

### **Scenario 2: Limited Download**
Download GraphQL data for new bookmarks:
```bash
python xmarks.py download --limit 100 --resume
```

### **Scenario 3: Full Pipeline**
Complete processing for new bookmarks file:
```bash
python xmarks.py full --bookmarks new-bookmarks.json
```

### **Scenario 4: YouTube Post-Processing**
Process existing cached tweets to pull YouTube metadata, optional embeds, and transcripts:
```bash
python xmarks.py youtube --limit 100 --no-resume
```

### **Scenario 5: Testing**
Test with small dataset:
```bash
python xmarks.py process --use-cache --limit 10 --verbose
```

---

## ğŸš¨ **Important Notes**

### **Authentication**
- Requires `twitter_cookies.json` with valid session
- Use browser developer tools to export cookies
- Test with small batches first

### **Rate Limiting**
- GraphQL: 45 requests per 15 minutes
- Processing from cache: No limits
- LLM APIs: Configurable delays

### **Resume Capability**
- Downloads skip cached tweets
- Processing skips existing files
- Safe to interrupt at any time

### **File Management**
- Media files use unique names to avoid collisions
- PDFs organized by type (ArXiv vs general)
- All documents linked with Obsidian wikilinks

---

## ğŸ› **Troubleshooting**

### **Common Issues**

| Issue | Solution |
|-------|----------|
| GraphQL fails | Check `twitter_cookies.json` validity |
| Rate limited | Wait 15 minutes or reduce batch size |
| LLM errors | Verify API keys in environment variables |
| Missing media | Check `knowledge_vault/media/` permissions |
| YouTube disabled | Set `YOUTUBE_API_KEY` and enable in config |
| Transcript server | For Whisper: ensure Ollama running; For Deepgram: set `DEEPGRAM_API_KEY` |
| API not reachable | Start `xmarks_api.py` (FastAPI) and check `/health` |

### **Debug Mode**
```bash
# Enable verbose logging
python xmarks.py process --verbose

# Check logs
tail -f xmarks.log
```

---

## ğŸ”® **Roadmap**

### **Completed** âœ…
- GraphQL engine with caching
- Modular processor architecture
- Thread detection and compilation
- URL expansion and PDF downloads
- ArXiv paper processing
- Media downloads with Obsidian links
- Multi-provider LLM integration
- Resume capability for all operations
 - YouTube processing (embeds + transcripts)
 - Live capture API server and browser extension

### **Planned** ğŸš§
- [ ] Plugin system for custom processors
- [ ] Web UI for monitoring progress
- [ ] Export to other formats (Notion, Roam)
- [ ] Incremental bookmark updates
- [ ] Vector database integration

---

## ğŸŒ API Server (Live Capture)

Start the API server to receive real-time bookmarks from the browser extension/userscript:

```bash
uvicorn xmarks_api:app --reload
```

Endpoints:

- `GET /health` â€“ health check
- `POST /api/bookmark` â€“ receive bookmark payloads
- `GET /api/bookmarks` â€“ recent captured bookmarks
- `POST /api/process` â€“ trigger processing of captured items
- `GET /api/stats` â€“ capture statistics

All writes to `realtime_bookmarks.json` are funneled through an async lock so concurrent browser submissions cannot corrupt the capture cache; prefer using these endpoints instead of editing the file by hand.

See `browser_extension/README.md` for installation and usage.

---

## ğŸ—„ï¸ Metadata Database (SQLite)

- Location: configurable via `database.path` (default `.xmarks/meta.db`)
- Tables: `tweets`, `url_mappings`, `downloads`, `llm_cache`, `files_index`, `graphql_cache_index`
- CLI: `python xmarks.py db stats|vacuum|export`
- Purpose: reuse across runs, speed resume, power browser/API lookups

---

## ğŸ§° Configuration Highlights

- Granular pipeline flags in `config.json` under `pipeline.stages`
- LLM prompts under `llm.prompts.*` (tags, summary, alt_text, readme, transcript)
- File naming under `files.naming_patterns.*` (tweets prefixed with `tweets_`)
- Downloads: timeouts/retries/user-agent under `downloads.*`
- Database: `database.enabled`, `database.path`, `database.wal_mode`

---

## ğŸ“„ **License**

MIT License - See LICENSE file for details

---

## ğŸ¤ **Contributing**

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Test thoroughly with small datasets
4. Submit a pull request

---

**Built with a focus on modularity, reliability, and comprehensive knowledge capture.**